[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Canadian Demographic and Housing Data",
    "section": "",
    "text": "This book is intended for people interested in learning how to access, process, analyze, and visualize Canadian demographic, economic, and housing data using R. The target audience we have in mind ranges from interested individuals interested in understanding their environment through data, community activists and community groups interested in introducing data-based approached into their work, journalists who want to report on data in their stories or aim to incorporate their own descriptive data analysis, non-profits or people involved in policy who are looking for data-based answers to their questions.\nThe most important prerequisite is a keen interest in using data to help understand how housing and demographics shape cities and rural areas in Canada, and a willingness to learn. Prior knowledge of R is not necessary, but may be beneficial.\nCanada has high quality demographic, economic and housing data. While significant data gaps exist, the available data often remains under-utilized in policy and planning analyses. Moreover, many analyses that do come out go quickly out of date and can’t easily be updated because they rely on non-reproducible and non-adaptable workflows.\nIn this book we will maintain a strong emphasis on reproducible and adaptable work flows to ensure the analysis is transparent, can easily be updated as new data becomes available, and can be tweaked or adapted to address related questions.\n\n\nThis book will take a project based approach to to teach through examples, with one project per section. Each project will be loosely broken up into four parts.\n\nFormulating the question. What is the question we are interested in? Asking a clear question will help focus our efforts and ensure that we don’t aimlessly trawl through data.\nIdentifying possible data sources. Here we try to identify data sources that can speak to our question. We will also take the time to read up on definitions and background concepts to better understand the data and prepare us for data analysis, and understand how well the concepts in the data match our original question from step 1.\nData acquisition. In this step we will import the data into our current working session. This could be as simple as an API call, or more complicated like scraping we table from the web, or involve even more complex techniques to acquire the data.\nData preparation. In this step we will reshape and filter the data to prepare it for analysis.\nAnalysis. This step could be as simple as computing percentages or even doing nothing, is the quantities we are interested in already come with the dataset, if our question can be answered by a simple descriptive analysis. In other cases, when our question is more complex, this step may be much more involved. The book will try to slowly build up analysis skills along the way, with increasing complexity of questions and required analysis.\nVisualization. The final step in the analysis process is to visualize and communicate the results. In some cases this can be done via a table or a couple of paragraphs of text explaining the results, but in most cases it is useful to produce graphs or maps or even interactive visualizations to effectively communicate the results.\nInterpretation. What’s left to wrap this up is to interpret the results. How does this answer our question, where does it fall short. What does this mean in the real-world context? What new questions emerge from this?\n\nWhile we won’t always follow this step by step process to the letter, it will be our guiding principle throughout the book. Sometimes things won’t go so clean, where after the visualization step we notice that something looks off or is unexpected, and we may jump back up a couple of steps and add more data and redo parts of the analysis to better understand our data and how it speaks to our initial questions. We might even come to understand that our initial question was not helpful or was ill-posed, and we will come back to refine it.\n\n\n\nBy taking this approach we have several goals in mind:\n\nStay motivated by using real world Canada-focused and (hopefully) interesting examples.\nTeach basic data literacy, appreciate definitions and quirks in the data.\nExpose the world of Canadian data and make it more accessible.\nLearn how data can be interpreted in different ways, and data and analysis is not necessarily “neutral”.\nLearn how to effectively communicate results.\nLearn how to adapt and leverage off of previous work to answer new questions.\nLearn how to reproduce and critique data analysis.\nBuild a community around Canadian data, where people interested in similar questions, or people using the same data, can learn from each other.\nRaise the level of understanding of Canadian data and data analysis so we are better equipped to tackle the problems Canada faces.\n\nThis is setting a very high goal for this book, and we are not sure we can achieve all of this. But we will try our best to be accessible and interesting as possible.\n\n\n\nMost people reading this book will not have used R before, or only used it peripherally, maybe during a college course many years in the past. Instead, readers may be familiar with working through housing and demographic data in Excel or similar tools. Or making maps in QGIS or similar tools when dealing with spatial data. And the type of analysis outlined above that this book will teach can in general terms be accomplished using these tools.\nBut where tools like spreadsheets and desktop GIS fall short is in another important focus of this book: transparency, reproducibility, and adaptability.\nAn analysis in a spreadsheet or desktop GIS typically involves a lot of manual steps, the work is not reproducible without repeating these steps. We can’t easily inspect how the result was derived, the analysis lacks transparency. When we just compute a ratio or percentage this may not be so bad, but trying to understand how a more complex analysis was done in a spreadsheet easily turns into a nightmare. Analysis that involves a lot of manual steps is not auditable without putting in the work to repeat those manual steps.\nBut why does this matter? It’s always been this way, some experts produce analysis and produce a glossy paper to present the results. One can argue if this was an adequate modus operandi in the past, but we feel strongly that it’s not in today’s world. The lines between experts and non-experts has become blurred, and the value we place on lived experience has increased relative to more formal expertise. We argue this places different demands on policy-relevant analysis, it needs to be open and transparent, in principle anyone should be able to understand how the analysis was done and the conclusions were reached. That’s where reproducibility and transparency come in. And it also requires bringing up data analysis skills in the broader population, so that the ability to reproduce and critique an analysis in principle can be realized in practice.\nThe remaining reason for using R, adaptability, has also become increasingly important. The amount of data available to us has increased tremendously, but our collective ability to analyse data and extract information has not kept up. Doing analysis in R allows us to efficiently reuse previous analysis to perform a similar one. Or to build on previous analysis to deepen it. Which turbocharges our ability to do analysis, covering more ground and going deeper.\nR is not the only framework to do this in, there are other options like python or julia. But we believe that R is best suited for people transitioning into this space, and we can rely on an existing ecosystem of packages to access and process Canadian data. People already proficient in python will have no problem translating what we do into their preferred framework, or dynamically switch back and forth between R, python or whatever other tools they prefer as needed and convenient.\n\n\n\nWhich brings us to our most ambitious goal, to help create a community around Canadian data analysis. When analysis is transparent, reproducible and adaptable people can piggy-back of each other’s work, reusing parts of analysis others have done and building and improving upon it. Or Critiquing and correcting analysis, or taking it toward a different direction. A community that grows in their understanding of data, and a community using a shared set of tools to access and process Canadian data, enabling discussions to move forward instead of in circles. A community that builds up expertise from the bottom up.\nThe book tries to address both of these requirements for building a Canadian data community, a principled approach to data and data analysis, while introducing R as a common framework to work in hoping that the reader will come away with\n\nbetter data literacy skills to understand and critique data analysis,\ntechnical skills to reproduce and perform their own data analysis, and\na common tool set for acquiring, processing and analyzing Canadian data that facilitates collaborative practices."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In this section we give a taste of what’s to come. Some of the concepts introduced in the preface may be too abstract to picture for people just starting out in this space. People probably grasp the importance of having a principled approach to data analysis, from formulating a question all the way to sharing results. But why so much emphasis on reproducibility and adaptability? And do we really need to learn a new framework like R for this?\nThis is best understood by walking through a simple example of what analysis of Canadian data in R, and a Canadian data community might look like. We won’t explain all steps in full detail here, this is to serve to illustrate the concepts talked above in the preface and give the reader a taste of what’s to come.\nIf you don’t understand all the code now, don’t worry, that’s part of the point of this book. We will work out and explain these examples in detail in the first chapter of the book. What’s important right now is to illustrate the principle of reproducible and adaptable code, and how this can function to foster a community of Canadian data analysis. And to note how little code is needed to make this work."
  },
  {
    "objectID": "intro.html#a-hypothetical-example",
    "href": "intro.html#a-hypothetical-example",
    "title": "1  Introduction",
    "section": "1.1 A hypothetical example",
    "text": "1.1 A hypothetical example\nImagine Amy, a Toronto-based social services worker looking to pilot a community intervention targeted at children in low income. She is in the process of putting together a proposal describing her intervention and is trying to locate a good neighbourhood for her pilot and make a compelling case to possible funders.\nAmy knows that census data has a good geographic breakdown of children in poverty, but the latest available data is from 2016, using 2015 income data. CRA tax data is available up to 2019, but also has information on families in low income, but nothing directly on children in the standard release tables at fine geographies. As a first step she settles on census data, with the goal to re-run the analysis once the 2021 data comes out later in the year.\nShe refers to the Census Dictionary to understand the various low income measures, and uses CensusMapper’s interactive map that allows to explore these concepts. She would have liked to use the Market Based Measure, but due to data availability she settles for LICO-AT.\nShe sets up a new Notebook and loads in the R libraries that she will need for this, ggplot2for graphing and cancensus for ingesting the data.\n\nlibrary(cancensus)\nlibrary(ggplot2)\n\nNext she pull in the data. the CensusMapper API GUI tool helps her locate the StatCan geographic identifier for Toronto, (3520005), and the internal CensusMapper vector for the percentage of children in LICO-AT (v_CA16_2573).\n\nlico_yyz <- get_census(\"CA16\",regions=list(CSD=\"3520005\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"CT\",geo_format=\"sf\")\n\nHere Amy specified that she wants data for the 2016 Canadian census (“CA16”), the region and vectors, at the census tract (“CT”) level, with geographies as well as the low income data.\nNow that she has the data at her finger times her first step is to make a map. For that she needs to tell ggplot is what variable to use as fill colour, and maybe give it a nicer colour scale and some labels to explain what the map is about.\n\nggplot(lico_yyz, aes(fill=(lico/100))) +\n  geom_sf() +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nBased on this she locates a couple of good candidate neighbourhoods for her pilot and sends the map in a email to her colleague Peter to get input on which neighbourhood might be best suited.\nPeter has some good feedback for Amy, but also gets an idea to try and set up something similar in Vancouver. Peter asks Amy if she can share the code, and Amy sends along the above code snippets. Peter looks up the geographic identifier for Vancouver and subs that in instead of Toronto’s.\n\nlico_yvr <- get_census(\"CA16\",regions=list(CSD=\"5915022\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"CT\",geo_format=\"sf\")\n\nggplot(lico_yvr, aes(fill=(lico/100))) +\n  geom_sf() +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nEasy peasy, thanks to Amy’s previous work. Peter takes the map to his friend Yuko and asks her for advice where a community-based intervention for low-income children might make sense in Calgary. Yuko asks for the code from Peter to take a closer look herself.\nYuko is interested in a finer geographic breakdown, so she swaps our the geographic level from census tracts to dissemination areas.\n\nlico_yvr_da <- get_census(\"CA16\",regions=list(CSD=\"5915022\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"DA\",geo_format=\"sf\")\n\nggplot(lico_yvr_da, aes(fill=(lico/100))) +\n  geom_sf(size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nBut then Yuko pauses to think that maybe looking at share of the low income population is not the right metric. She decides to query the number of children in low income (vector “v_CA16_2558”) and prepare the data for a dot-density map.\n\n# remotes::install_github(\"mountainmath/dotdensity\")\nlibrary(dotdensity)\n\nlico_dots_yvr <- get_census(\"CA16\",regions=list(CSD=\"5915022\"),geo_format=\"sf\",\n                            vectors=c(lico=\"v_CA16_2558\"), level=\"DA\") |>\n  compute_dots(\"lico\")\nyvr_city <- get_census(\"CA16\",regions=list(CSD=\"5915022\"),geo_format=\"sf\")\n\nggplot(lico_dots_yvr) +\n  geom_sf(data = yvr_city) +\n  geom_sf(size=0.25,colour=\"brown\",alpha=0.1) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nThat paints a somewhat different picture, and Yuko feels this is much better suited to pinpoint where to best stage a community intervention. She lets Peter and Amy know and emails them her modifications to the code.\nMeanwhile, Yuko’s Vancouver friend Stephanie is looking specifically at children below the age of 6 in low income, and wants to understand how the geographic distribution of low income children has changed over time. Comparing census data through time can be tricky because census geographies change, but this problem has been completely solved via the tongfen R package. Looking at Yuko’s work she thinks it might be best to look at both, the change in share of children in low income as well as the change in absolute number.\n\nlibrary(tongfen)\nmeta <- meta_for_ca_census_vectors(c(total_2006=\"v_CA06_1982\",lico_share_2006=\"v_CA06_1984\",\n                                     lico_2016=\"v_CA16_2561\",lico_share_2016=\"v_CA16_2576\"))\n\nlico_data <- get_tongfen_ca_census(regions=list(CSD=\"5915022\"),meta,level=\"CT\") |>\n  mutate(lico_2006=total_2006*lico_share_2006/100) |>\n  mutate(`Absolute change`=lico_2016-lico_2006,\n         `Percentage point change`=lico_share_2016-lico_share_2006)\n\nArmed with this data Stephanie can plot the absolute and percentage point change in children below 6 in low income.\n\nggplot(lico_data,aes(fill=`Absolute change`)) +\n  geom_sf() +\n  scale_fill_gradient2() +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in number of children under 6 in low income\",\n       caption=\"StatCan Census 2006, 2016\")\n\n\n\n\n\nggplot(lico_data,aes(fill=`Percentage point change`/100)) +\n  geom_sf() +\n  scale_fill_gradient2(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in share of children under 6 in low income\",\n       fill=\"Percentage\\npoint change\",\n       caption=\"StatCan Census 2006, 2016\")\n\n\n\n\nStephanie shares her results with Amy in Toronto in case there are components of Amy’s pilot specifically targeting children below 6 in low income.\nMeanwhile Amy has been trying to understand more broadly how the share of low income children has evolved since the 2016 census (using 2015 income data) at the metropolitan level over longer time spans, so she looks through the StatCan socioeconomic tables and settles on table 11-10-0135, which also allows her to compare various low income concepts.\n\nlibrary(cansim)\nmbm_timeline <- get_cansim(\"11-10-0135\") |>\n  filter(`Persons in low income`==\"Persons under 18 years\",\n         GEO==\"Toronto, Ontario\",\n         Statistics==\"Percentage of persons in low income\")\n\nggplot(mbm_timeline,aes(x=Date,y=val_norm,colour=`Low income lines`)) +\n  geom_point(shape=21) +\n  geom_line() +\n  scale_y_continuous(labels=scales::percent) +\n  labs(title=\"Children in low income in Metro Toronto\",\n       y=\"Share of children in low income\",\n       x=NULL,\n       caption=\"StatCan Table 11-10-0135\")\n\n\n\n\nShe notes that there has been a substantial overall drop in children in low income since 2015 across all measures, which is excellent news. She considers pushing off her pilot project until after the 2021 census data comes out to first understand if the geographic patterns have changed."
  },
  {
    "objectID": "intro.html#what-you-will-learn-in-this-book",
    "href": "intro.html#what-you-will-learn-in-this-book",
    "title": "1  Introduction",
    "section": "1.2 What you will learn in this book",
    "text": "1.2 What you will learn in this book\nLooking at R code for the first time can be intimidating. If the code looks opaque right now, there is no need to worry. It will be explained in detail in the first chapter and is very much part of the rationale for writing this book. If decisions around what low income metric to pick, or why tongfen is needed to compare census data through time are not clear, again, that will be explained in this book in detail and expanding understanding of data and data analysis is the other big rationale for this book.\nReaders will learn how to reproduce analysis, how to critique analysis, and adapt it for their own purposes. And readers will learn how to conduct their own analysis in the Canadian context, based on questions and use cases relevant to them.\nHopefully the above hypothetical scenario have explained how the adaptability of the R code has made life much easier for several of the subsequent analysis steps, and how little code was needed to gain some insights and communicate results."
  },
  {
    "objectID": "intro/intro_to_r.html",
    "href": "intro/intro_to_r.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Statistics Canada produces a lot of high quality demographic and economic data for Canada. CMHC complements this with housing data, and municipalities across Canada often provide relevant data through their Open Data portals."
  },
  {
    "objectID": "intro/intro_to_r.html#r-and-rstudio",
    "href": "intro/intro_to_r.html#r-and-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "R and RStudio",
    "text": "R and RStudio\nWe will be working in R and the RStudio IDE, although using a different editor like Visual Studio Code works just as well, especially if you are already familiar with it. Within R we will be operating within the tidyverse framework, a group of R packages that work well together and allow for intuitive operations on data via pipes.\nWhile an introduction to R is part of the goal of this book, an we will slowly build up skills as we go, we not give a systematic introduction but rather build up skills slowly as we work on concrete examples. It may be beneficial to supplement this with a more principled introduction to R and the tidyverse."
  },
  {
    "objectID": "intro/intro_to_r.html#packages",
    "href": "intro/intro_to_r.html#packages",
    "title": "Getting started with R and RStudio",
    "section": "Packages",
    "text": "Packages\nPackages are bundled sets of functionality that expand base R. We install or upgrade packages with the install.packages`. For example, to install the tidyverse framework we type\n\ninstall.packages(\"tidyverse\")\n\ninto the R console. This will install or upgrade the package and required dependencies. To make the functionality, for example the tibble function from the tibble package that is part of tidyverse, available to use we can then either access functions from the package using the :: namespace selector tibble::tibble() or first load the tibble or tidyverse package via library(tidyverse) that makes the tibble() function available without having to use the namespace selector."
  },
  {
    "objectID": "intro/intro_to_r.html#basic-data-manipulation-patterns",
    "href": "intro/intro_to_r.html#basic-data-manipulation-patterns",
    "title": "Getting started with R and RStudio",
    "section": "Basic data manipulation patterns",
    "text": "Basic data manipulation patterns\nThere are several basic data manipulation patterns that we will use throughout, and we want to give a quick overview using the Palmer Penguins dataset from the palmerpenguins package.\n\n# install.packages(\"palmerpenguins\") # install the package if needed\nlibrary(palmerpenguins)\n\nNow we have all the functionality of the palmerpenguins package available.\n\nExploring the data\nWith the palmerpenguins package comes the penguins dataset, we can expect the first few rows using the head() function which displays the first few rows.\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nThe str() function offers another convenient way to get an overview over the data.\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nWe can also type View(penguins) into the console to view the dataset in a spreadsheet form.\n\n\nBasic data manipulation\nTo manipulate and visualize the data we load the tidyverse package.\n\nlibrary(tidyverse)\n\nWe will explore some common data manipulation and visualization workflows.\n\nCount groups\nTo see how many rows there are for each species we ‘pipe’ the penguins dataset into the count() verb. Pipes are how we can stepwise transform data, the pipe operator is given by %>% withing the tidyverse framework and now also available natively in base R via |>. These two function (almost) the same way, and we will use both in this book.\n\npenguins |> count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nThis gives us the count of each species in the dataset, the pipe |> inserts the left hand side as the first argument in the count() function. We could have equivalently written this without the pipe operator as count(penguins,species).\n\n\nGroup and summarize\nThe usefulness of the pipe operator becomes clear when we chain several data transformation. If we want to know the mean bill length by species, we group by species and summarize the data.\n\npenguins |> \n  group_by(species) |>\n  summarize(bill_length_mm=mean(bill_length_mm, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  species   bill_length_mm\n  <fct>              <dbl>\n1 Adelie              38.8\n2 Chinstrap           48.8\n3 Gentoo              47.5\n\n\nHere we explicitly specify how missing values should be treated when summarizing, na.rm=TRUE says that NA values should be ignored when computing the mean.\n\n\n\nVisualizing data\nWe can visualize the data using ggplot. For this we have to specify the mapping aesthetics, we plot the bill length on the x-axis, the depth on the y-axis, colour by species and plot the data as points. The labs() function allows us to customize the graph labels.\n\nggplot(penguins,aes(x=bill_length_mm,y=bill_depth_mm,colour=species)) +\n  geom_point() +\n  labs(title=\"Penguin bill length vs depth\",\n       x=\"Bill length  (mm)\",y=\"Bill depth (mm)\",\n       colour=\"Penguin species\",\n       caption=\"Palmer Station Antarctica LTER\")\n\n\n\n\n\nAdd regression lines\nAs an aside we note the Simpson’s paradox, in the overall dataset the bill depth declines with length, but if we look separately within each species the bill depth increases with bill length. To make that explicit we can add regression lines using the geom_smooth function using lm (linear model) as the smoothing method.\n\nggplot(penguins,aes(x=bill_length_mm,y=bill_depth_mm,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  geom_smooth(method=\"lm\", colour=\"black\") +\n  labs(title=\"Penguin bill length vs depth\",\n       x=\"Bill length  (mm)\",y=\"Bill depth (mm)\",\n       colour=\"Penguin species\",\n       caption=\"Palmer Station Antarctica LTER\")\n\n\n\n\nThe first geom_smooth() function will add a regression line for each species, distinguished by colour in the plot aesthetics. Overriding the colour argument in the second geom_smooth() function will forget that the data was coloured by species and add the black regression line run on the entire dataset.\n\n\n\nMore data manipulations\nThere are several common data manipulation steps that we will employ frequently.\n\nFiltering rows\nOften we are only interested in subsets of the data, we can filter the rows in the dataset by using the filter verb from the dplyr package that is part of tidyverse. For example, if we want to take the previous plot but only show it for penguins on the island of Biscoe we can filter the data accordingly before plotting it.\n\npenguins |>\n  filter(island==\"Biscoe\") |>\nggplot(aes(x=bill_length_mm,y=bill_depth_mm,colour=species)) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  geom_smooth(method=\"lm\", colour=\"black\") +\n  labs(title=\"Penguin bill length vs depth\",\n       subtitle=\"Biscoe island only\",\n       x=\"Bill length  (mm)\",y=\"Bill depth (mm)\",\n       colour=\"Penguin species\",\n       caption=\"Palmer Station Antarctica LTER\")\n\n\n\n\n\n\nSelecting columns\nInstead of filtering rows It can be useful to select a subset of the columns to remove columns we don’t need and de-clutter the dataset. This is especially useful when producing tables. If we want a table of the numeric data fields of all female Adelie penguins on the island of Biscoe observed in 2007 we can filter by sex and island and select the columns we want.\n\npenguins |>\n  filter(island==\"Biscoe\", sex==\"female\", species==\"Adelie\", year==2007) |>\n  select(where(is.numeric),-year) \n\n# A tibble: 5 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           <dbl>         <dbl>             <int>       <int>\n1           37.8          18.3               174        3400\n2           35.9          19.2               189        3800\n3           35.3          18.9               187        3800\n4           40.5          17.9               187        3200\n5           37.9          18.6               172        3150\n\n\n\n\nMutating data\nWe often want to change data fields, or compute new columns from existing ones. For example, if we want to convert the body mass from g to kg we can add a new column using mutate for that.\n\npenguin_selection <- penguins |>\n  filter(island==\"Biscoe\", sex==\"female\", species==\"Adelie\", year==2007) |>\n  mutate(body_mass_kg=body_mass_g/1000) |>\n  select(where(is.numeric),-year,-body_mass_g)\n\npenguin_selection\n\n# A tibble: 5 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_kg\n           <dbl>         <dbl>             <int>        <dbl>\n1           37.8          18.3               174         3.4 \n2           35.9          19.2               189         3.8 \n3           35.3          18.9               187         3.8 \n4           40.5          17.9               187         3.2 \n5           37.9          18.6               172         3.15\n\n\n\n\nPivoting data\nThe data in our penguin_selection dataset above is in wide form, all the different variables are in their own column. Often it is useful to convert it to long form, where we only have one value column with the numeric values and another column specifying the type of measurement. In this case it is useful to add an identification column so that we know which measurements belong to the same penguin. We can just label the penguins by row number.\n\npenguin_selection_long <- penguin_selection |>\n  mutate(ID=row_number()) |>\n  pivot_longer(-ID,names_to=\"Metric\",values_to=\"Value\")\n\npenguin_selection_long |> head()\n\n# A tibble: 6 × 3\n     ID Metric            Value\n  <int> <chr>             <dbl>\n1     1 bill_length_mm     37.8\n2     1 bill_depth_mm      18.3\n3     1 flipper_length_mm 174  \n4     1 body_mass_kg        3.4\n5     2 bill_length_mm     35.9\n6     2 bill_depth_mm      19.2\n\n\nWe can do the reverse transformation, going from long form to wide form, using pivot_wider.\n\npenguin_selection_long |>\n  pivot_wider(names_from = Metric,values_from = Value)\n\n# A tibble: 5 × 5\n     ID bill_length_mm bill_depth_mm flipper_length_mm body_mass_kg\n  <int>          <dbl>         <dbl>             <dbl>        <dbl>\n1     1           37.8          18.3               174         3.4 \n2     2           35.9          19.2               189         3.8 \n3     3           35.3          18.9               187         3.8 \n4     4           40.5          17.9               187         3.2 \n5     5           37.9          18.6               172         3.15\n\n\nThis recovers the previous form of the data, with the added ID column."
  },
  {
    "objectID": "intro/intro_to_r.html#canadian-data-packages",
    "href": "intro/intro_to_r.html#canadian-data-packages",
    "title": "Getting started with R and RStudio",
    "section": "Canadian data packages",
    "text": "Canadian data packages\nDuring the course of this book we will make heavy use of several R packages to facilitate data access to Canadian data, we will introduce them in this chapter."
  },
  {
    "objectID": "intro/intro_cansim.html",
    "href": "intro/intro_cansim.html",
    "title": "2  Introduction to cansim",
    "section": "",
    "text": "The cansim R package interfaces with the StatCan NDM that replaces the former CANSIM tables. It can be queried for\n\nwhole tables\nspecific vectors\ndata discovery searching through tables\n\nIt encodes the metadata and allows to work with the internal hierarchical structure of the fields.\nLarger tables can also be imported into a local SQLite database for reuse across sessions without the need to re-download the data, and better performance when subsetting the data or performing other basic data operations at the database level before loading the data into memory.\nData discovery is can be cumbersome, the list_cansim_cubes function from the cansim package fetches the newest list of all available tables and can be filtered by survey, release date or dates of data coverage. The table list is cached for the duration of the R session. The seach_cansim_cubes function provides a convenient shortcut to narrow down this list.\nIn some cases searching the web for “StatCan Table xxxx”, where “xxxx” contains search phrases for the data of interest, is sometimes a useful way to discover data. In reverse, we can bring up the StatCan webpage for a specific table number using the view_cansim_webpage function and explore the data via the web interface. Especially for large datasets this can be a faster way to determine if a specific table contains the information we are interested in without first having to download the data.\nTo get overview information for a table we have already downloaded the get_cansim_table_overview function provides a high-level overview over the variables contained in the table. The get_cansim_column_list function returns a list of the available columns or dimensions in the table, and get_cansim_column_categories returns the list of levels in a specific dimension. The get_cansim_table_nots provides the data notes that can hold important information to guide interpretation of some of the dimensions or levels.\nThe data is accessed via get_cansim function, or alternatively the get_cansim_sqlite function that stores the data permanently for use across R sessions in a local SQLite database. By default the English language tables are accessed, setting the language=\"fr\" parameter changes that to the French version. The SQLite option is especially useful for larger tables. The cansim package will emit a warning if an SQLite table is outdated and newer data is available, if the auto_refresh=TRUE option is passed to the function call it will automatically download any new data if available. When accessing data from the SQLite version we can use normal dplyr verbs to filter the data or perform basic select, group_by or summarize operations before calling collect_and_normalize to fetch the result from the database and enrich it with metadata.\nMetadata added by the cansim package includes converting the dimension values to factors and adding information on the hierarchical structure of the levels. Moreover, the package creates a native Date field and a val_norm field with normalized values. The values shipped by StatCan are sometimes expressed in “thousands of units”, the val_norm converts this to base units for easier interpretation and uniformity across tables.\nMore information can be found in the package documentation and the package vignettes."
  },
  {
    "objectID": "intro/intro_cancensus.html",
    "href": "intro/intro_cancensus.html",
    "title": "3  Introduction to cancensus",
    "section": "",
    "text": "The cancensus R package interfaces with the CensusMapper API server. It can be queried for\n\ncensus geographies for census years 1996 through 2021\ncensus profile data for census years 1996 through 2021\nsome census custom tabulations\nhierarchical metadata of census variables\nsome non-census data that comes on census geographies, e.g. T1FF taxfiler data\n\nA slight complication, the cancensus packageneeds an API key. You can sign up for one for free on CensusMapper.\nOnce you have your API key it’s useful to store it as an environment variable in your .Renviron configuration file so it’s available in all your R sessions.\n\ncancensus::set_api_key(key = \"CensusMapper_XXXX...XXXX\", install=TRUE)\n\nBy default {cancensus} cached downloaded census data, which makes it easier and faster to re-run analysis and protects from overusing the CensusMapper API quota. To use caching an local path needs to be designated for data caching. The cache is shared across R sessions.\n\ncancensus::set_cache_path(cache_path = \"<local path ot cache data>\", install=TRUE)\n\n{cancensus} provides convenient access to census data. Calls to {cancensus} require to spectify\n\nThe dataset, for example “CA21” for the 2021 Canadian census. A list of available datasets can be accessed via cancensus::list_census_datasets().\nThe regions to access the data for, this is a list keyed by geographic levels. For example, to access data for the Vancouver census metropolitan area it would be list(CMA=\"59933\"), for the City of Toronto it would be list(CSD=\"3520005\"). Region parameters can contain several regions of the same type or mix regions of different type. For example, to access data for the region covered by the Vancouver School Board, we need to assemble two CSDs and three CTs list(CSD=c(\"5915022\",\"5915803\"),CT=c(\"9330069.04\",\"9330069.03\",\"9330069.02\")). This allows pinpointing what geographic region we are interested in.\nThe geographic level to query the data for. This simply be the regions specified in the regions parameter, but it could also be any geographic level equal to or lower than the lowest level geographic region specified in the regions parameter. Valid level identifiers are DB for dissemination blocks, DA for dissemination areas, EA for enumeration areas for the 1996 census, CT for census tracts, CSD for census subdivisions, CMA for census metropolitan areas or census agglormerations, CD for census districts, PR for provinces or territories and C for country level data. Geographic regions can also be assembled using the CensusMapper API GUI tool, CSD and higher level geographies can be explored or searched programmatically via the list_census_regions() or search_census_regions() functions.\nThe vectors parameter allows to specify which census variables to query. By default the data comes with population, dwelling and household counts, other census variables can be explored and selected via the CensusMapper API GUI tool or explored or searched programmatically via the list_census_vectors() or find_census_vectors() functions. There are also helper function to select variables using the internal CensusMapper metadata and hierarchy of census variables via the child_census_vectors() function. For finer control over the names of the names of the returned census variable the vectors parameter can be a named vector.\nThe geo_format parameters allows to select if geographic data should also be downloaded, and if yes, in what format. In this post we will only access data via the modern “sf” format, but data can also be accessed in the legacy “sp” spatial data format.\n\nAs an example we will retried the share of the population in Toronto, Mississauga, and Brampton spending 30% or more of income on shelter costs in 2016.\n\nlibrary(cancensus)\nlibrary(dplyr)\nregions <- list(CSD=c(\"3520005\",\"3521005\",\"3521010\"))\nvectors <- c(shelter_cost_burdened=\"v_CA16_4889\", shelter_base = \"v_CA16_4886\")\n\ndata <- get_census(dataset = \"CA16\", regions=regions, vectors=vectors)\ndata %>% \n  mutate(`Share shelter cost burdened`=shelter_cost_burdened/shelter_base) |>\n  select(GeoUID,`Region Name`,`Share shelter cost burdened`)\n\n# A tibble: 3 × 3\n  GeoUID  `Region Name`    `Share shelter cost burdened`\n  <chr>   <fct>                                    <dbl>\n1 3520005 Toronto (C)                              0.296\n2 3521005 Mississauga (CY)                         0.264\n3 3521010 Brampton (CY)                            0.305"
  },
  {
    "objectID": "intro/intro_cmhc.html",
    "href": "intro/intro_cmhc.html",
    "title": "4  Introduction to cmhc",
    "section": "",
    "text": "The cmhc R package interfaces with the CMHC Housing Market Information Portal (HMIP) and allows programmatic and reproducible access to CMHC data. This gives access to data from four major CMHC surveys\n\nStarts and Completions Survey (Scss), which has data on housing construction covering starts, completions, units under construction, length of construction, absorbed and unabsorbed units and their prices.\nRental Market Survey (Rms), which surveys the purpose-built rental market on an annual (and for some time twice-annual) basis. It has data on vacancy rates, availability rates, rents, fixed sample rent change and the overall rental universe by bedroom type, structure size, and year of construction.\nSecondary Rental Market Survey (Srms), which covers parts of the secondary market rental market with data on condominium apartment vacancy ratesm rents, and number and share of rented units, as well as some information on other secondary rentals.\nSenior’s housing (Seniors), which gives data on seniors housing of various levels of assistance.\nCensus data (Census), which holds several housing related cross-tabulations.\n\nThe package is designed to work in conjunction with the cancensus package and census geographic identifiers.\nThe nature of the CMHC backend makes it at times challenging to find data, the cmhc package has several convenience functions to facilitate data discovery. The list_… functions, for example list_cmhc_surveys() list options. The select_cmhc_table() allows the interactive selection of data tables in the console, and returns the syntax for the desired function call to acquire the data."
  },
  {
    "objectID": "intro/tongfen.html",
    "href": "intro/tongfen.html",
    "title": "5  tongfen",
    "section": "",
    "text": "The tongfen R package facilitates making data on different geometries comparable.\nTongFen (通分) means to convert two fractions to the least common denominator, typically in preparation for further manipulation like addition or subtraction. In English, that's a mouthful and sounds complicated. But in Chinese there is a word for this, TongFen, which makes this process appear very simple.\nWhen working with geospatial datasets we often want to compare data that is given on different regions. For example census data and election data. Or data from two different censuses. To properly compare this data we first need to convert it to a common geography. The process to do this is quite analogous to the process of TongFen for fractions, so we appropriate this term to give it a simple name. Using the tongfen package, preparing data on disparate geographies for comparison by converting them to a common geography is as easy as typing tongfen.\nIn particular, the package has a number of convenience functions to facilitate making Canadian census data comparable through time, making it easy to perform longitudinal analysis on fine geographies based on the Canadian Census. Essentially, the tongfen package creates a semi-custom tabulation based on Dissemination Block, Dissemination Area, or Census Tract geographies.\nThese semi-custom tabulations are created in three steps:\n\nCreate a correspondence table for geographies from different censuses. By default the official StatCan correspondence files are used for that, but these only exist back to 2001 when the current geographic system based on dissemination blocks and dissemination areas started. To go back further, when enumeration areas were the basic building block, we need to rely on geospatial matching of the areas to create a harmonized common geography.\nCreate metadata that contains information about how the census variables of interest can be aggregated in the case where geographies get joined. For example, if we are interested in the share of households in low income, we need to know what this share is based on in order to correctly aggregate it up. CensusMapper holds detailed metadata, so this process is automated.\nJoin geographies and aggregate census data as described in the correspondence table from Step 1 and the metadata in step 2.\n\nThe result of this process is a semi-custom tabulation of the data we want that is created on the fly, at the price of coming on a slightly coarser geography than the original input geographies in cases where geographies had to be joined to create the harmonized geography."
  },
  {
    "objectID": "basic_descriptive/basic_descriptive.html",
    "href": "basic_descriptive/basic_descriptive.html",
    "title": "Basic descriptive analysis",
    "section": "",
    "text": "The accompanying analysis won’t be very involved, sometimes we will compute percentages or make other simple data manipulations, but generally the analysis will be quite straight-forward. We will focus on how to find data sources that can inform on our question, how to get the data, and how to present and interpret it."
  },
  {
    "objectID": "basic_descriptive/cerb.html",
    "href": "basic_descriptive/cerb.html",
    "title": "6  Geography of CERB",
    "section": "",
    "text": "In 2020 Canada introduced the COVID-19 Emergency Recovery Benefit (CERB), a program to support people with who lost income during the pandemic."
  },
  {
    "objectID": "basic_descriptive/cerb.html#question",
    "href": "basic_descriptive/cerb.html#question",
    "title": "6  Geography of CERB",
    "section": "6.1 Question",
    "text": "6.1 Question\nWhere did CERB benefits go?"
  },
  {
    "objectID": "basic_descriptive/cerb.html#data-sources",
    "href": "basic_descriptive/cerb.html#data-sources",
    "title": "6  Geography of CERB",
    "section": "6.2 Data sources",
    "text": "6.2 Data sources\nStandard T1FF taxfiler data has this for large geographies, to understand fine geographic distribution we turn to Census data from the 2021 census, which reports on 2020 income. The census dictionary explains\n\nCanada Emergency Response Benefit (CERB) payments received during the reference period. This benefit was intended to provide financial support to employees and self‑employed Canadians who had lost their job or were working fewer hours due to the COVID‑19 pandemic and the public health measures implemented to minimize the spread of the virus.\n\nCensus income data is taken directly from T1 tax returns and linked at the individual person level."
  },
  {
    "objectID": "basic_descriptive/cerb.html#data-acquisition",
    "href": "basic_descriptive/cerb.html#data-acquisition",
    "title": "6  Geography of CERB",
    "section": "6.3 Data acquisition",
    "text": "6.3 Data acquisition\nWe can use the CensusMapper API tool and search for “COVID-19” in the Variable Selection tab to locate available census variables. Since we are interested in where people lived that received the benefit we select v_CA21_593, the number of recipients, as well as v_CA21_554, the baseline of people 15 years or older who are in principle eligible for this benefit.\nWe also need to decide which region we want to investigate, let’s take a look at the City of Ottawa. We can select the city in the Region Selection tab and read off the geographic identifier 3506008 for the City of Ottawa in the Overview tab.\nNow we have all we need to pull in the data, we just need to decide on the geographic granularity. Let’s use census tracts, a standard geographic region aiming to capture between 2,500 and 7,500 people in metropolitan areas. We also specify that we want the geographies, not just the tabular data.\n\nlibrary(cancensus)\nottawa_cerb <- get_census(\"CA21\",regions=list(CSD=\"3506008\"),\n                          vectors=c(cerb=\"v_CA21_593\", base=\"v_CA21_554\"),\n                          level=\"CT\", geo_format=\"sf\")"
  },
  {
    "objectID": "basic_descriptive/cerb.html#data-preparation",
    "href": "basic_descriptive/cerb.html#data-preparation",
    "title": "6  Geography of CERB",
    "section": "6.4 Data preparation",
    "text": "6.4 Data preparation\nTo understand the geographic distribution we compute the percentage of people 15 years and over receiving CERB. Generally in this book we work in the tidyverse to help with data manipulation and visualization, so we load that library too.\n\nlibrary(tidyverse)\nplot_data <- ottawa_cerb |>\n  mutate(Share=cerb/base)\n\nThere is not much to do, computing a percentage is a simple division. The mutate verb creates a new column called Share holding the computed ratios."
  },
  {
    "objectID": "basic_descriptive/cerb.html#analysis-and-visualization",
    "href": "basic_descriptive/cerb.html#analysis-and-visualization",
    "title": "6  Geography of CERB",
    "section": "6.5 Analysis and visualization",
    "text": "6.5 Analysis and visualization\nAll that’s left is to visualize the data. To plot geographic data we use ggplot and the geom_sf geometry. We need to tell it how to colour the map, the aesthetic, and we specify to fill each area by the share of CERB recipients.\n\nggplot(plot_data) +\n  geom_sf(aes(fill=Share))\n\n\n\n\nTo make this a little nicer we add labels, remove the coordinate grid and choose nicer colours and reduce the boundary line size.\n\nggplot(plot_data) +\n  geom_sf(aes(fill=Share), size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"CERB recipients in the City of Ottawa\",\n       fill=\"Share of people\\n15+ reveiving\\nCERB\",\n       caption=\"StatCan Census 2021\")\n\n\n\n\nIt is difficult to see the central parts, we might want to zoom in a little. At the same time, it might be useful to add in Gatineau and surrounding municipalities, so maybe we want the data for the entire metro area.\nTo do this we copy the code from above and paste it into a single pip, from data acquisition (using the CMA 505 for Ottawa CMA) and cut the region to the central parts by looking at the grid from the first map.\n\nget_census(\"CA21\",regions=list(CMA=\"505\"),\n                          vectors=c(cerb=\"v_CA21_593\", base=\"v_CA21_554\"),\n                          level=\"CT\", geo_format=\"sf\") |>\n  mutate(Share=cerb/base) |>\n  ggplot() +\n  geom_sf(aes(fill=Share), size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA, xlim=c(-76,-75.5), ylim=c(45.3,45.5)) +\n  labs(title=\"CERB recipients in Ottawa\",\n       fill=\"Share of people\\n15+ reveiving\\nCERB\",\n       caption=\"StatCan Census 2021\")\n\n\n\n\nThis brings out the central regions much better. We could also try this with finer geographies, setting the level to dissemination areas instead of census tracts. The same code as before works, except changing the level=\"CT\" to level=\"DA\".\n\nget_census(\"CA21\",regions=list(CMA=\"505\"),\n                          vectors=c(cerb=\"v_CA21_593\", base=\"v_CA21_554\"),\n                          level=\"DA\", geo_format=\"sf\") |>\n  mutate(Share=cerb/base) |>\n  ggplot() +\n  geom_sf(aes(fill=Share), size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA, xlim=c(-76,-75.5), ylim=c(45.3,45.5)) +\n  labs(title=\"CERB recipients in Ottawa\",\n       fill=\"Share of people\\n15+ reveiving\\nCERB\",\n       caption=\"StatCan Census 2021\")"
  },
  {
    "objectID": "basic_descriptive/cerb.html#interpretation",
    "href": "basic_descriptive/cerb.html#interpretation",
    "title": "6  Geography of CERB",
    "section": "6.6 Interpretation",
    "text": "6.6 Interpretation\nWe notice substantial differences in the share of people receiving CERB benefits, with rural areas generally having lower shares, and central areas being more mixed, varying between under 10% to well over 40% of people 15 years and over receiving CERB. Generally areas with lower incomes have benefited more from CERB."
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html",
    "href": "basic_descriptive/cars_vs_suvs.html",
    "title": "7  Cars vs SUVs in Canada",
    "section": "",
    "text": "Private motor vehicles in Canada seem to be getting larger and it feels like SUVs and light trucks are taking over. This subjective feeling prompts us to ask the following question to check if this is just our imagination or a real phemominon."
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#question",
    "href": "basic_descriptive/cars_vs_suvs.html#question",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.1 Question",
    "text": "7.1 Question\nAre SUVs and light trucks taking over in Canada?\nThis question is somewhat vague, it’s not clear what taking over means. But the question is clear enough to get us started on some descriptive analysis."
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#data-sources",
    "href": "basic_descriptive/cars_vs_suvs.html#data-sources",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.2 Data sources",
    "text": "7.2 Data sources\nData discovery can be challenging, but just typing “statcan motor vehicle sales” into a search engine is a good start and gets us to the StatCan table enumerating data on new motor vehicle sales. We can also use the built-in search functionality in the {cansim} package.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cansim)\n\nsearch_cansim_cubes(\"motor vehicle sales\") |>\n  select(cansim_table_number,cubeTitleEn,cubeStartDate,cubeEndDate)\n\n# A tibble: 2 × 4\n  cansim_table_number cubeTitleEn                          cubeStar…¹ cubeEndD…²\n  <chr>               <chr>                                <date>     <date>    \n1 20-10-0001          New motor vehicle sales              1946-01-01 2022-07-01\n2 20-10-0002          New motor vehicle sales, by type of… 2010-01-01 2021-01-01\n# … with abbreviated variable names ¹​cubeStartDate, ²​cubeEndDate\n\n\nThere are two tables with motor vehicle sales, we can inspect them on the web or via the {cansim} package. The second table covers a much shorter time period, and is also less recent. We will check out the first table to see if it fits our needs.\nTo access the web we can simply type view_cansim_webpage(\"20-10-0001\") into the console, which will open the StatCan webpage for Table 20-10-0001. Getting table overview data via the {cansim} package requires to load the table first, which can be slow for larger tables.\n\nget_cansim_table_overview(\"20-10-0001\")\n\nThis tells us that Table 20-10-0001 might have the information we need, we check the table notes to better understand what “Trucks” entails, selecting the two columns we are interested in.\n\nget_cansim_table_notes(\"20-10-0001\") %>% \n  select(`Member Name`,Note) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\nMember Name\nNote\n\n\n\n\nNA\nPrior to 1953, data for Canadian and United States manufactured vehicles and overseas manufactured vehicles are not segregated.\n\n\nBritish Columbia and the Territories\nIncludes Yukon, Northwest Territories and Nunavut.\n\n\nTrucks\nTrucks include minivans, sport-utility vehicles, light and heavy trucks, vans and buses.\n\n\nTotal, overseas\nIncludes Japan and other countries.\n\n\nNA\nSeasonally adjusted data for the New Motor Vehicle Sales survey are available for the period between January 1991 to February 2012.\n\n\n\n\n\nIt looks like “Trucks” does includes SUVs, but next to light trucks it also includes heavy trucks and buses. It also includes minivans, and thinking back at our original question, we might want to refine it to include minivans.\nThis allows us to separate passenger cars from basically everything else. Thinking that heavy truck and bus sales probably only make up a small portion, we could use that as a stand-in for our “SUVs and light trucks” in our question. But the match is not ideal and this leaves questions open.\nMaybe Table 20-10-0002 works better for our purposes, time to look at the table overview.\n\nget_cansim_table_overview(\"20-10-0002\")\n\nThe frequency is only annual as opposed to the monthly data from the previous table, but the breakdown of vehicle types looks much better for our purposes, it allows us to distinguish light trucks from heavy trucks and buses. Time to check the table notes for more details on the definitions.\n\nget_cansim_table_notes(\"20-10-0002\") %>% \n  select(`Member Name`,Note) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\nMember Name\nNote\n\n\n\n\nBritish Columbia and the Territories\nIncludes Yukon, Northwest Territories and Nunavut.\n\n\nTrucks\nTrucks include minivans, sport-utility vehicles, light and heavy trucks, vans and buses.\n\n\nLight trucks\nLight trucks: include minivans, sport-utility vehicles, light trucks and vans.\n\n\nHeavy trucks\nHeavy trucks: include class 4, 5, 6, 7 and 8 trucks.\n\n\n\n\n\nThis looks like it fits what we need, we want to compare unit sales of Passenger cars to Light trucks."
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#data-acquisition",
    "href": "basic_descriptive/cars_vs_suvs.html#data-acquisition",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.3 Data acquisition",
    "text": "7.3 Data acquisition\nGetting the data is easy now. The {cansim} package will automatically add a native Date column, to convert annual data to dates it defaults to July 1st of that year. While it is a sensible default to assign a mid-year date to annual data, later on for plotting it will be more convenient for us to set the date at January 1st, so we change override the default using the optional default_month argument.\n\ndata <- get_cansim(\"20-10-0002\", default_month = 1)\n\ndata |> select(Date,GEO,`Vehicle type`,Sales,val_norm) %>%\n  head()\n\n# A tibble: 6 × 5\n  Date       GEO    `Vehicle type`            Sales      val_norm\n  <date>     <chr>  <fct>                     <fct>         <dbl>\n1 2010-01-01 Canada Total, new motor vehicles Units       1584499\n2 2010-01-01 Canada Total, new motor vehicles Dollars 52315609000\n3 2010-01-01 Canada Passenger cars            Units        710214\n4 2010-01-01 Canada Passenger cars            Dollars 18982437000\n5 2010-01-01 Canada Trucks                    Units        874285\n6 2010-01-01 Canada Trucks                    Dollars 33333173000\n\n\nQuick inspection of the data, using the columns we identified in the overview, helps identify the basic structure of the data."
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#data-preparation",
    "href": "basic_descriptive/cars_vs_suvs.html#data-preparation",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.4 Data preparation",
    "text": "7.4 Data preparation\nThere is not much data preparation needed, we just filter down to the data we are interested in.\n\nplot_data <- data |>\n  filter(`Vehicle type` %in% c(\"Passenger cars\",\"Light trucks\"),\n         Sales==\"Units\",\n         GEO==\"Canada\")"
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#analysis-and-visualization",
    "href": "basic_descriptive/cars_vs_suvs.html#analysis-and-visualization",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.5 Analysis and visualization",
    "text": "7.5 Analysis and visualization\nTime to lake a look what this looks like, to plot we filter for the overall Canadian data series, and tell ggplot to map Date on the x-axis, the values colum `VALUE` on the y-axis, and colour by vehicle type.\n\nggplot(plot_data,aes(x=Date,y=VALUE,colour=`Vehicle type`)) +\n  geom_line() \n\n\n\n\nThis is looking good, time to clean up the graph a bit. We add markers for the data points, nicer axis labels, as well as a title and labels.\n\nggplot(plot_data,aes(x=Date,y=VALUE,colour=`Vehicle type`)) +\n  geom_point(shape=21) +\n  geom_line() +\n  scale_y_continuous(labels=scales::comma) +\n  scale_color_manual(labels=c(\"Light trucks\"=\"Light trucks, SUVs,\\nminvans and vans\"),\n                     values=c(\"Light trucks\"=\"brown\",\"Passenger cars\"=\"steelblue\")) +\n  labs(title=\"New motor vehicle sales in Canada\",\n       x=NULL,y=\"Annual number of vehicles.\",\n       caption=\"StatCan Table 20-10-0002\")\n\n\n\n\nThis answers our question in that more light trucks, SUVs, minivans and vans are sold than cars, and the gap has been growing. But the data only starts in 2010, and we suspect that things weren’t always this way. At what point did SUVs and light trucks overtake new car sales?\nTo answer than we need to jump back and load the other time series. It won’t let us separate out heavy trucks and buses, but we can estimate how bad the difference is by comparing it to this data.\nWe load the data and filter it down to the parts that we are interested in.\n\ndata2 <- get_cansim(\"20-10-0001\")\n\nplot_data2 <- data2 |>\n  filter(`Vehicle type` %in% c(\"Passenger cars\",\"Trucks\"),\n         Sales==\"Units\",\n         `Origin of manufacture`==\"Total, country of manufacture\",\n         `Seasonal adjustment`==\"Unadjusted\",\n         GEO==\"Canada\")\n\nA quick plot gives us a general idea what this looks like.\n\nplot_data2 %>%\n  ggplot(aes(x=Date,y=VALUE,colour=`Vehicle type`)) +\n  geom_line()\n\n\n\n\nThere is a strong seasonal pattern in vehicle sales, for now we will just aggregate it to annual sales so we can compare it with the previous data. For this we extract they Year from the Date column, group by Year and Vehicle type and summarize by adding up the `VALUE` column. We added a count column to keep track how many months we added up so we can later ensure we are only showing years for which we have complete data.\n\nplot_data2_annual <- plot_data2 |>\n  mutate(Year=strftime(Date,\"%Y\")) |>\n  group_by(Year,`Vehicle type`) |>\n  summarise(VALUE=sum(VALUE), n=n(),.groups=\"drop\") |>\n  mutate(Date=as.Date(paste0(Year,\"-01-01\"))) |>\n  filter(n==12)  # only use years with full 12 months of data\n\nggplot(plot_data2_annual,aes(x=Date,y=VALUE,colour=`Vehicle type`)) +\n  geom_line()\n\n\n\n\nTime to combine this with our previous data. This tells us that the most interesting change happened 1985 and onward, so we will discard earlier years. One quick sanity check is to see if the annual passenger car sales derived from the two series agree for the years where they are in common. Here we join the two data tables by Date and Vehicle type in order to compare the two estimate. We rename the VALUE column on the first one in order to avoid name conflicts.\n\nplot_data %>%\n  filter(`Vehicle type`==\"Passenger cars\") %>%\n  select(Date,`Vehicle type`,VALUE1=VALUE) %>%\n  left_join(plot_data2_annual,by=c(\"Date\",\"Vehicle type\")) %>%\n  mutate(diff=VALUE1-VALUE) %>%\n  select(Year,VALUE1,VALUE,diff)\n\n# A tibble: 12 × 4\n   Year  VALUE1  VALUE  diff\n   <chr>  <dbl>  <dbl> <dbl>\n 1 2010  710214 710214     0\n 2 2011  691079 691079     0\n 3 2012  759024 759024     0\n 4 2013  760924 760920     4\n 5 2014  760449 760449     0\n 6 2015  712322 712322     0\n 7 2016  661088 661088     0\n 8 2017  646960 646960     0\n 9 2018  586357 586357     0\n10 2019  496851 496851     0\n11 2020  325494 325494     0\n12 2021  345350 345350     0\n\n\nThe data for all years agrees, except for 2013 where one series counts 4 more passenger cars.\n\nbind_rows(plot_data %>% filter(`Vehicle type`!=\"Passenger cars\"),\n          plot_data2_annual %>% filter(Date>=as.Date(\"1985-01-01\"))) %>%\n  ggplot(aes(x=Date,y=VALUE,colour=`Vehicle type`)) +\n  geom_point(shape=21) +\n  geom_line() +\n  scale_y_continuous(labels=scales::comma) +\n  scale_color_manual(labels=c(\"Light trucks\"=\"Light trucks, SUVs,\\nminvans and vans\",\n                              \"Trucks\"=\"Trucks, SUVs,\\nminivans, vans and buses\"),\n                     values=c(\"Light trucks\"=\"brown\",\"Passenger cars\"=\"steelblue\",\n                              \"Trucks\"=\"purple\")) +\n  theme(legend.position = \"bottom\") +\n  labs(title=\"New motor vehicle sales in Canada\",\n       x=NULL,y=\"Annual number of vehicles.\",\n       caption=\"StatCan Tables 20-10-0001, 20-10-0002\")"
  },
  {
    "objectID": "basic_descriptive/cars_vs_suvs.html#interpretation",
    "href": "basic_descriptive/cars_vs_suvs.html#interpretation",
    "title": "7  Cars vs SUVs in Canada",
    "section": "7.6 Interpretation",
    "text": "7.6 Interpretation\nThis confirms our initial suspicion that the “Trucks” category is dominated by light trucks, SUVs, minivans and vans, at least for the years 2010 onwards where we have data for both. Which gives us confidence to say that truck and SUV sales caught up to passenger cars sales by around 1997, and the two evolved fairly parallel until 2009, after which SUVs and light trucks increased dramatically and passenger car sales fell."
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html",
    "href": "basic_descriptive/geography_of_income_change.html",
    "title": "8  Geography of income change",
    "section": "",
    "text": "Incomes in a region change by people getting higher (or lower) incomes as well as people moving in and out of a region. We can observe the aggregate effects by looking at change in income statistics."
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#question",
    "href": "basic_descriptive/geography_of_income_change.html#question",
    "title": "8  Geography of income change",
    "section": "8.1 Question",
    "text": "8.1 Question\nWhere and how did incomes change in the City of Vancouver?"
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#data-sources",
    "href": "basic_descriptive/geography_of_income_change.html#data-sources",
    "title": "8  Geography of income change",
    "section": "8.2 Data sources",
    "text": "8.2 Data sources\nThe main data sources for fine-geography income data is the census, although custom tabulations of T1FF taxfiler data can offer insight of this on an annual basis at the census tract geography. For our question we are interested in broad temporal ranges, so the 5-year census data will work well.\nWe need to decide which income concept is best suited for our question, it is worthwhile to spend some time with the Census Income Reference Guide to understand how the data was collected and what income concept to use. Prior to 2011 the income data was part of the long form census. In 2011 the mandatory long form was replaced with the voluntary NHS, given people the option to link directly to T1FF taxfiler data or to detail the income data manually. Starting 2016 income data was linked for all people to the T1FF taxfiler data.\nThe question what income concept to use, e.g. individual income, household income, family income, employment income, etc, depends on the particular question we are interested in. For now we will go with family income, trying to understand how the income situation of families varies across Vancouver and across time. Family income is less affected by demographic factors like the distribution of single vs multiple person households, but is still impacted by e.g. differences in shares of seniors vs young families vs families at the peak of their earnings."
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#data-acquisition",
    "href": "basic_descriptive/geography_of_income_change.html#data-acquisition",
    "title": "8  Geography of income change",
    "section": "8.3 Data acquisition",
    "text": "8.3 Data acquisition\nWe again use the CensusMapper API tool to locate the internal CensusMapper identifiers for Median Total Income of Economic Families for the years 2006 through 2021. For 2001 the standard census products reported income for census families instead of economic families, so they aren’t directly comparable. As geographic breakdown we choose census tracts.\n\nlibrary(tidyverse)\nregions <- list(CSD=\"5915022\")\nincome_vectors <- c(\"2021\"=\"v_CA21_965\",\n                    \"2016\"=\"v_CA16_2447\",\n                    \"2011\"=\"v_CA11N_2456\",\n                    \"2006\"=\"v_CA06_1741\")\n\nTo facilitate the data import we write a wrapper function to acquire the census data for each of our four years. For a given census year we create the corresponding dataset identifier and select the appropriate income variable. to reduce clutter we select just the income variable and also keep the geographic identifier, and add the census year to the table.\n\nget_census_data <- function(year){\n  year <- as.character(year)\n  dataset <- paste0(\"CA\",substr(year,3,4))\n  get_census(dataset,regions=regions,\n             vectors=c(\"ef_income\"=as.character(income_vectors[year])),\n             geo_format=\"sf\",level=\"CT\") |>\n    select(GeoUID,ef_income) |>\n    mutate(Year=year)\n}\n\nImporting the data is easy now, we just call our function for each census year and collect it into a data frame.\n\nlibrary(cancensus)\nincome_data <- seq(2006,2021,5) |>\n  map_df(get_census_data) \n\nLet’s take a quick look.\n\nggplot(income_data) +\n  geom_sf(aes(fill=ef_income),size=0.1) +\n  scale_fill_viridis_c(option=\"inferno\", labels=scales::dollar) +\n  facet_wrap(~Year) +\n  coord_sf(datum=NA) +\n  labs(title=\"Median economic family income\",\n       fill=\"Current dollars\",\n       caption=\"StatCan Census 2006-2021\")"
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#data-preparation",
    "href": "basic_descriptive/geography_of_income_change.html#data-preparation",
    "title": "8  Geography of income change",
    "section": "8.4 Data preparation",
    "text": "8.4 Data preparation\nLooking at the above graph we can see the geographic variation in each year, but it is difficult to discern geographic trends over time as incomes have gone up a lot during this timeframe. It makes sense to look at inflation-adjusted incomes instead. For this we use annual consumer price index data from StatCan Table 18-10-0005. To simplify things we locate the specific vector v41693271 for the all-time CPI.\n\nlibrary(cansim)\ninflation <- get_cansim_vector(\"v41693271\") |>\n  mutate(Year=strftime(Date,\"%Y\")) |>\n  select(Year,CPI=val_norm) |>\n  filter(Year %in% names(income_vectors))\n\ninflation\n\n# A tibble: 4 × 2\n  Year    CPI\n  <chr> <dbl>\n1 2006   109.\n2 2011   120.\n3 2016   128.\n4 2021   142."
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#analysis-and-visualization",
    "href": "basic_descriptive/geography_of_income_change.html#analysis-and-visualization",
    "title": "8  Geography of income change",
    "section": "8.5 Analysis and visualization",
    "text": "8.5 Analysis and visualization\nWith this, we can adjust the census data by inflation. We choose to base everything on 2021 dollars.\n\ninflation <- inflation |>\n  mutate(CPI=CPI/last(CPI,order_by = Year))\n\nNow we just join the inflation data onto our income data by year, this adds the CPI column from the inflation data farme to our income with the CPI value corresponding to the value in the Year column in each of the two data frames. We then colour by inflation-adjusted income using the same code for graphing as above.\n\nincome_data |>\n  left_join(inflation,by=\"Year\") |>\n  ggplot() +\n  geom_sf(aes(fill=ef_income/CPI),size=0.1) +\n  scale_fill_viridis_c(option=\"inferno\", labels=scales::dollar) +\n  facet_wrap(~Year) +\n  coord_sf(datum=NA) +\n  labs(title=\"Median economic family income\",\n       fill=\"Constant 2021\\ndollars\",\n       caption=\"StatCan Census 2006-2021\")\n\n\n\n\nThis shows more clearly how incomes have increased over time, but it would be nice to compute the change in income 2006 to 2021 for each individual census tract. But keen observers will notice that some census tracts have changed over the years, making it very difficult to compare data directly."
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#data-acquisition-part-2",
    "href": "basic_descriptive/geography_of_income_change.html#data-acquisition-part-2",
    "title": "8  Geography of income change",
    "section": "8.6 Data acquisition (part 2)",
    "text": "8.6 Data acquisition (part 2)\nFortunately the problem of making census data comparable across time has been solved with the tongfen package. This allows us to create a semi-custom tabulation on the fly on a harmonized geography based on census tracts by aggregating census data appropriately. One problem is that medians can’t be aggregated, so we need to either use average income instead or be content that medians can only be approximated. By default the tongfen package aggregates medians as if they were averages and emits a warning. This is the route we will take for this.\nTo start out, we need to create metadata for the tongfen procedure. This is automated for Canadian census data, leveraging the metadata built into CensusMapper.\n\nlibrary(tongfen)\nmeta <- meta_for_ca_census_vectors(income_vectors)\n\nmeta\n\n# A tibble: 8 × 10\n  variable     label      dataset type  aggre…¹ units rule  parent geo_d…²  year\n  <chr>        <chr>      <chr>   <chr> <chr>   <chr> <chr> <chr>  <chr>   <int>\n1 v_CA21_965   2021       CA21    Orig… Median… Curr… Medi… v_CA2… CA21     2021\n2 v_CA16_2447  2016       CA16    Orig… Median… Curr… Medi… v_CA1… CA16     2016\n3 v_CA11N_2456 2011       CA11N   Orig… Median… Curr… Medi… v_CA1… CA11     2011\n4 v_CA06_1741  2006       CA06    Orig… Median… Curr… Medi… v_CA0… CA06     2006\n5 v_CA21_964   v_CA21_964 CA21    Extra Additi… <NA>  Addi… <NA>   CA21     2021\n6 v_CA16_2446  v_CA16_24… CA16    Extra Additi… <NA>  Addi… <NA>   CA16     2016\n7 v_CA11N_2455 v_CA11N_2… CA11N   Extra Additi… <NA>  Addi… <NA>   CA11     2011\n8 v_CA06_1729  v_CA06_17… CA06    Extra Additi… <NA>  Addi… <NA>   CA06     2006\n# … with abbreviated variable names ¹​aggregation, ²​geo_dataset\n\n\nThe metadata contains our original income data, as well as extra variables needed to properly aggregate the data. Getting the income data on a common geography is easy now.\n\nunified_income_data <- get_tongfen_ca_census(regions,meta)"
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#analysis-and-visualization-1",
    "href": "basic_descriptive/geography_of_income_change.html#analysis-and-visualization-1",
    "title": "8  Geography of income change",
    "section": "8.7 Analysis and visualization",
    "text": "8.7 Analysis and visualization\nIn line with what we did before we want look at inflation-adjusted income change. To this end we extract the adjustment factor for the 2006-2021 timeframe.\n\ninflation_2006_2021 <- inflation |>\n  filter(Year==\"2006\") |>\n  pull(CPI)\n\nWith that we can simply plot the data, mapping the inflation-adjusted percent change 2006 to 2021.\n\nunified_income_data |>\n  ggplot() +\n  geom_sf(aes(fill=`2021`/`2006`*inflation_2006_2021-1),size=0.1) +\n  scale_fill_viridis_c(option=\"cividis\", labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in economic family income\",\n       fill=\"Change 2006-2021\\n(inflation adjusted)\",\n       caption=\"StatCan Census 2006-2021\")"
  },
  {
    "objectID": "basic_descriptive/geography_of_income_change.html#interpretation",
    "href": "basic_descriptive/geography_of_income_change.html#interpretation",
    "title": "8  Geography of income change",
    "section": "8.8 Interpretation",
    "text": "8.8 Interpretation\nIn summary we see that income of economic families changed fasted in the Downtown Eastside, Grandview-Woodlands and Strathcona neighbourhoods, effectively doubling. Incomes increased least on the West Side, where they were already quite high to start with, and increased by about 50% throughout much of the East Side."
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive.html",
    "href": "advanced_descriptive/advanced_descriptive.html",
    "title": "Advanced descriptive analysis",
    "section": "",
    "text": "Building on the section of basic descriptive analysis we will move into more advanced data processing and descriptive analysis. This will involve mixing of different datasets to tease out finer aspects. We will learn how to group and summarize data, and how to use joins."
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html",
    "title": "9  BC migration",
    "section": "",
    "text": "This example is motivated by a BC government press release titled “B.C. welcomes more than 100,000 people – the most in 60 years”. This is the type of attention-grabbing headline where our gut reaction usually is to question if this is true.\nLet’s first try and understand what the headline really means. B.C. “welcoming” people refers to people moving to the province from elsewhere, either from other provinces or internationally. So this is referring to gross in-migration. But reading the text of the press release it immediately pivots to a different concept, saying that “B.C.’s net migration reached 100,797 people in 2021”. It helpfully explains that net migration is the difference between people moving here and people moving away. Which is quite different from the number of people B.C. “welcomed” that year, or the number of people “moving to the province in 2021” as implied by the title and the first sentence of the press release.\nSo here comes the first difficulty, the press release is contradicting itself by mixing two concepts. That leads us to formulate a fairly broad question that should help clear this up."
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#question",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#question",
    "title": "9  BC migration",
    "section": "9.1 Question",
    "text": "9.1 Question\nHow many people has B.C. welcomed, net and gross, how has that changed over the last 6 decades, and how should this be interpreted?"
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-sources",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-sources",
    "title": "9  BC migration",
    "section": "9.2 Data sources",
    "text": "9.2 Data sources\nTo start, let’s figure out where that data point comes from.\nThe press release references StatCan as the source, let’s search through the StatCan tables. Google usually works reasonably well, but we can also search programmatically. We are looking for migration estimates from the quarterly demographic estimates to get the most up-to-data population estimates from StatCan. For results we just need the first two columns, that table number and the title.\n\nlibrary(tidyverse)\nlibrary(cansim)\n\nsearch_cansim_cubes(\"migration\") |> \n  filter(grepl(\"quarterly\",cubeTitleEn)) |>\n  arrange(desc(cubeEndDate)) |> \n  select(1:2)\n\n# A tibble: 2 × 2\n  cansim_table_number cubeTitleEn                                               \n  <chr>               <chr>                                                     \n1 17-10-0020          Estimates of the components of interprovincial migration,…\n2 17-10-0040          Estimates of the components of international migration, q…\n\n\nIt looks like Table 17-10-0020 and 17-10-0040 are what we are looking for. Let’s load in the data and inspect the first couple of rows for BC."
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-acquisition",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-acquisition",
    "title": "9  BC migration",
    "section": "9.3 Data acquisition",
    "text": "9.3 Data acquisition\n\ninterprovincial <- get_cansim(\"17-10-0020\")\ninternational <- get_cansim(\"17-10-0040\")\n\ninterprovincial |>\n  filter(GEO==\"British Columbia\") |>\n  select(GEO,Date,`Interprovincial migration`,val_norm) |>\n  tail()\n\n# A tibble: 6 × 4\n  GEO              Date       `Interprovincial migration` val_norm\n  <chr>            <date>     <fct>                          <dbl>\n1 British Columbia 2021-10-01 In-migrants                    12183\n2 British Columbia 2021-10-01 Out-migrants                    8987\n3 British Columbia 2022-01-01 In-migrants                    15970\n4 British Columbia 2022-01-01 Out-migrants                   13154\n5 British Columbia 2022-04-01 In-migrants                    29347\n6 British Columbia 2022-04-01 Out-migrants                   25053\n\n\nFor inter-provincial migration we get in and out migration counts for every quarter. Let’s also inspect the international migration data.\n\ninternational |>\n  filter(GEO==\"British Columbia\") |>\n  select(GEO,Date,`Components of population growth`,val_norm) |>\n  tail()\n\n# A tibble: 6 × 4\n  GEO              Date       `Components of population growth` val_norm\n  <chr>            <date>     <fct>                                <dbl>\n1 British Columbia 2022-01-01 Net non-permanent residents           3344\n2 British Columbia 2022-04-01 Immigrants                           15990\n3 British Columbia 2022-04-01 Emigrants                             1959\n4 British Columbia 2022-04-01 Returning emigrants                   1676\n5 British Columbia 2022-04-01 Net temporary emigrants               1294\n6 British Columbia 2022-04-01 Net non-permanent residents          26942\n\n\nHere we get immigrants, emigrants, returning emigrants, but for temporary emigrants and non-permanent residents we only get net change. That puts a bit of a damper on our ambition to look at gross migration, for those last two categories net is all we have."
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-preparation",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#data-preparation",
    "title": "9  BC migration",
    "section": "9.4 Data preparation",
    "text": "9.4 Data preparation\nNext we got to wrangle this data into a useful format. We are interested in all of these components, so we need to join these two data series together. We will retain the GeoUID, GEO, Components of population growth, Date and val_norm columns, which requires some renaming and then defining factor levels so that they stack nicely later in our plots. We also flip the sign on out-migrants and emigrants, as these are out-flows. To make sure those two time series start at the same time we cut it off appropriately.\nThe press release talked about annual change, so we do a rolling sum over 4 quarters, right-aligning the data so it’s for the period of the preceding year.\n\nmigration_data <- bind_rows(\n  interprovincial |> \n    select(GeoUID,GEO,Date,\n           `Components of population growth`=`Interprovincial migration`,val_norm) |>\n    mutate(`Components of population growth`=\n             paste0(\"Interprovincial \",tolower(`Components of population growth`))),\n  international |> \n    select(GeoUID,GEO,Date,`Components of population growth`,val_norm)\n) |>\n  mutate(`Components of population growth`=\n           factor(`Components of population growth`,\n                  levels=c(\"Interprovincial out-migrants\",\n                           \"Emigrants\",\n                           \"Interprovincial in-migrants\",\n                           \"Immigrants\",\n                           \"Returning emigrants\",\n                           \"Net temporary emigrants\",\n                           \"Net non-permanent residents\"))) |>\n  mutate(value=ifelse(`Components of population growth` %in% \n                        c(\"Interprovincial out-migrants\",\"Emigrants\"),\n                      -val_norm,val_norm)) |>\n  filter(Date>=pmax(min(interprovincial$Date),min(international$Date))) |>\n  group_by(GeoUID,`Components of population growth`) |>\n  arrange(Date) |>\n  mutate(annual=zoo::rollsum(value,k=4,na.pad = TRUE,align = \"right\")) |>\n  filter(!is.na(annual)) |>\n  ungroup()\n\nWe will also need net migration stats, so let’s compute these by summing of the components,\n\nnet_migration <- migration_data |> \n  group_by(Date,GEO,GeoUID) |>\n  summarize(value=sum(value),annual=sum(annual),.groups=\"drop\") |>\n  mutate(`Components of population growth`=\"Net migration\")"
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#analysis-and-visualization",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#analysis-and-visualization",
    "title": "9  BC migration",
    "section": "9.5 Analysis and visualization",
    "text": "9.5 Analysis and visualization\nTime to make a graph.\n\nmigration_colours <- setNames(MetBrewer::met.brewer(\"Archambault\",7),\n                              migration_data$`Components of population growth` %>% \n                                levels %>% rev)\n\nggplot(migration_data |> filter(GEO==\"British Columbia\")) +\n  geom_area(aes(x=Date,y=annual,fill=fct_rev(`Components of population growth`)),\n           stat=\"identity\") +\n  scale_y_continuous(labels=scales::comma) +\n  geom_line(data=net_migration |> filter(GEO==\"British Columbia\"),\n            aes(x=Date,y=annual)) +\n  scale_fill_manual(values=migration_colours) +\n  labs(title=\"BC Year over year migration\",\n       y=\"Year over year change\",x=NULL,fill=\"Components of population change\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040\")\n\n\n\n\nThis shows us that the press report was most likely talking did not mean to talk about number of people B.C. has “welcomed” or that “moved to the province” but instead the difference between the number of people it welcomed and the number of people it bid farewell.\nAnd the net migration is indeed at record levels. At least in absolute terms. But B.C. now is very different from B.C. in the 60s at the start of this time series. How can we compare net migration over time in a more meaningful way? Normalizing by population is a good option here. Let’s grab the data and take a look how B.C. population has changed.\n\npop_data <- get_cansim(\"17-10-0009\") |>\n  select(GEO,Date,Population=val_norm)\n\npop_data |> \n  filter(GEO==\"British Columbia\") |>\n  ggplot(aes(x=Date,y=Population)) +\n  geom_line() +\n  scale_y_continuous(labels=scales::comma) +\n  labs(title=\"Population estimates for British Columbia\",\n       y=\"Number of people\",\n       x=NULL,\n       caption=\"StatCan Table 17-10-0009\")\n\n\n\n\nIndeed, the trend is quite strong. Let’s fold that in and normalize by population.\n\nmigration_data |> \n  left_join(pop_data, by=c(\"GEO\",\"Date\")) |>\n  filter(GEO==\"British Columbia\") |>\n  ggplot() +\n  geom_area(aes(x=Date,y=annual/Population,fill=fct_rev(`Components of population growth`)),\n           stat=\"identity\") +\n  scale_y_continuous(labels=scales::percent) +\n  geom_line(data=net_migration |> \n              left_join(pop_data, by=c(\"GEO\",\"Date\")) |>\n              filter(GEO==\"British Columbia\"),\n            aes(x=Date,y=annual/Population)) +\n  scale_fill_manual(values=migration_colours) +\n  labs(title=\"BC Year over year BC Year over year migration\",\n       y=\"Year over year change per population\",x=NULL,\n       fill=\"Components of population change\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040, 17-10-0009\")\n\n\n\n\nHere the picture looks a little different. Net migration per capita is at its highest since the 90s, but the past 60 years there were several periods where it was larger.\nThe press report also mentioned that B.C.’s interprovincial migration numbers are higher than any other province. This is easy to check now.\n\nmigration_data_interprovinicial <- migration_data |>\n  left_join(pop_data, by=c(\"GEO\",\"Date\")) |>\n  filter(grepl(\"Interprovincial\",`Components of population growth`))\n\nnet_interprovincial <- migration_data_interprovinicial |>\n  group_by(GEO,Date) |>\n  summarize(value=sum(value),\n            annual=sum(annual),\n            Population=first(Population),\n            .groups=\"drop\")\n\nmigration_data_interprovinicial |> \n  filter(GEO!=\"Canada\") |>\n  filter(Date==max(Date)) |>\n  ggplot(aes(y=GEO,x=annual)) +\n  geom_bar(stat=\"identity\",\n           aes(fill=fct_rev(`Components of population growth`))) +\n  geom_boxplot(data=net_interprovincial |> \n                 filter(GEO!=\"Canada\") |>\n                 filter(Date==max(Date))) +\n  scale_fill_manual(values=migration_colours[grepl(\"Interprov\",names(migration_colours))]) +\n  scale_x_continuous(labels=scales::comma) +\n  labs(title=\"Most recent year migration\",\n       fill=\"Components of population growth\",\n       y=NULL,x=\"Year over year change\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040, 17-10-0009\")\n\n\n\n\nIn absolute number B.C. indeed has both the highest interprovincial in-migration and interprovincial net-migration among all provinces. But the provinces have vastly different sizes, so that’s not really a fair comparison. Again, we can normalize by population.\n\nmigration_data_interprovinicial |> \n  filter(GEO!=\"Canada\") |>\n  filter(Date==max(Date)) |>\n  ggplot(aes(y=GEO,x=annual/Population)) +\n  geom_bar(stat=\"identity\",\n           aes(fill=fct_rev(`Components of population growth`))) +\n  geom_boxplot(data=net_interprovincial |> \n                 filter(GEO!=\"Canada\") |>\n                 filter(Date==max(Date))) +\n  scale_fill_manual(values=migration_colours[grepl(\"Interprov\",names(migration_colours))]) +\n  scale_x_continuous(labels=scales::percent) +\n  labs(title=\"Most recent year migration\",\n       fill=\"Components of population growth\",\n       y=NULL,x=\"Year over year change per capita\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040, 17-10-0009\")\n\n\n\n\nViewed this way B.C.’s interprovincial in-migration and net migration still looks good, but many of the other provinces beat out that growth rate.\nFor completeness we can also just show the full graph that includes the international migration components.\n\nmigration_data |> \n  left_join(pop_data, by=c(\"GEO\",\"Date\")) |>\n  filter(Date==max(Date)) |>\n  ggplot(aes(y=GEO,x=annual/Population)) +\n  geom_bar(stat=\"identity\",aes(fill=fct_rev(`Components of population growth`))) +\n  geom_boxplot(data=net_migration |> \n                 left_join(pop_data, by=c(\"GEO\",\"Date\")) |> \n                 filter(Date==max(Date))) +\n  scale_fill_manual(values=migration_colours) +\n  scale_x_continuous(labels=scales::percent) +\n  labs(title=\"Most recent year migration\",\n       fill=\"Components of population growth\",\n       y=NULL,x=\"Year over year change per capita\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040, 17-10-0009\")"
  },
  {
    "objectID": "advanced_descriptive/advanced_descriptive_bc_migration.html#interpretation",
    "href": "advanced_descriptive/advanced_descriptive_bc_migration.html#interpretation",
    "title": "9  BC migration",
    "section": "9.6 Interpretation",
    "text": "9.6 Interpretation\nThis answers our question, the latest annual net migration edges over the 100,000 people mark, and in absolute terms this is the highest it’s been over at least 60 years. And B.C.’s interprovincial (net) in-migration was the highest in Canada in absolute terms. But what can we learn from that?\nB.C. 60 years ago is very different from B.C. today. To account for that we can normalize by population, and the relative net migration has been higher at several times during the past 60 years, most recently in the 90s.\nWe also note that big dip in net-migration during COVID-19. It is not clear if the current heights are a bounce-back to make up for the comparatively low net in-migration during the pandemic, or if it is simply reverting back to the increasing trend we have seen over the past 10 years."
  }
]